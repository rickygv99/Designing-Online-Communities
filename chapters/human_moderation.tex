\documentclass[class=book, crop=false]{standalone}

%% Image paths
\usepackage{graphicx}
\graphicspath{{images/}}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{csquotes}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Sets epigraph style
\usepackage{epigraph}
\setlength\epigraphwidth{.8\textwidth}
\setlength\epigraphrule{0pt}

%% Sets line style
\linespread{1.3}

%% Key term command
\usepackage{marginnote}
\providecommand{\keyterm}[1]{\textbf{#1}\marginnote{\scriptsize \textbf{#1}}}

\begin{document}

\epigraph{\itshape “It’s not about being right one hundred percent of the time, it’s about being able to execute effectively.”}{---\textit{Radiolab, "Post No Evil"}}

\epigraph{\itshape "People sleep peaceably in their beds at night only because rough men stand ready to do violence on their behalf."}{---George Orwell}

As a social system develops, it attracts a larger and larger user base. With that larger user base comes bad actors. They post offensive and sometimes even illicit content, disrupting the community and even driving good members of the community off the social system. In order to counteract these bad actors, a social system needs moderators\index{moderation}. Moderation isn't a glamorous job. As one Facebook employee describes, “There is a sewer channel and all of the mess/dirt/waste/sh*t of the world flow[s] towards you and you have to clean it.” [Chen 2012, gawker.com] To illustrate this point, let's take a look at the case of Microsoft employee Henry Soto.

Henry Soto was born and raised in Texas, where he became interested in computers and met Sara Soto, who he married in 2003. Sara Soto was also interested in computers; they were a good match. During his career, Henry moved between different computer companies but he hoped to eventually work at Microsoft\index{Microsoft}. When Sara was hired by a Microsoft vendor in 2005, the Sotos moved to Washington State, and Henry became newly excited that he might be also employed by Microsoft. He applied for various job openings at Microsoft and landed a temporary job on Microsoft's emergency response team. In 2006, he was hired as a Microsoft vendor and then fully employed by Microsoft in 2007. He was promoted several times, eventually becoming a Service Delivery Manager, where he managed Microsoft's call center and fixed problems with MSN.com.

At the same time as Henry was becoming employed by Microsoft, Microsoft was becoming increasingly worried about their online services being overrun by bad actors -- people who post pornography, hateful messages, illegal content, and so on. In 2008, Microsoft was spurred into action by a new law which required online companies to report crimes such as child pornography to the National Center for Missing and Exploited Children (NCMEC). To comply with this law, Microsoft needed some form of \keyterm{moderation}.

Microsoft created the Online Safety Team\index{Microsoft!Online Safety Team}, a subdivision of its Customer Service division. The Online Safety Team was responsible for reviewing content posted on Microsoft's online services, such as Outlook and Bing, and removing any negative content, reporting it to law enforcement if necessary. This form of moderation, where humans are paid to moderate content, is defined by Western University professor Sarah T. Roberts as \keyterm{commercial content moderation}\index{moderation!commercial content moderation} (CCM) [Roberts 2016].

Henry Soto was one of the Microsoft employees who were assigned to the Online Safety Team. Henry didn't realize the psychological toll the position might take on his mental health and only knew that he was supposed to police violations of Microsoft's terms and conditions. As a member of the Online Safety Team, Henry had to look through horrible photos and videos in order to filter them out. Microsoft recognized the difficulty Henry and others faced; in one performance review, Henry was praised for his "courage".

However, Henry began to have sleep problems, nightmares and anxiety. He wasn't the only one; other employees had mental breakdowns at work. He began meeting with psychiatrists but, as his work continued, his problems only worsened. He began to hallucinate and had trouble interacting with children, including his son, or computers because he would recall the horrific child abuse he had seen. Henry had \keyterm{secondary traumatic-stress disorder}\index{secondary traumatic-stress disorder}, which has symptoms similar to post traumatic-stress disorder (PTSD). 

In 2017, Henry Soto filed a lawsuit against Microsoft along with fellow employee Greg Blauert. They sought compensation for the psychological harm they continue to face and the difficulty they have finding employment due to their severe mental problems. They alleged that Microsoft didn't warn them about the psychological toll the job would take on them and failed to provide adequate mental support resources, such as meetings with psychologists and a spousal wellness program, to employees in the Online Safety Team.\index{Microsoft!Online Safety Team}

Social systems aren't required to moderate in many countries, including the United States. According to the \keyterm{principle of safe harbor}\index{principle of safe harbor}, under U.S. law, social systems have the right but not the responsibility to moderate content in their communities. [Gillespie 2018] However, whether social systems have responsibility for their content ultimately depends on the region in which they operate. From a legal perspective, many social systems wish to do business in as many regions as possible, and so they must provide moderation to do so. However, from a business perspective, since providing moderation creates a better community and thus makes users more likely to use the social system, it makes economic sense to provide moderation.

\section{Commercial content moderators}

When people hear "moderation," typically the first thing they think of is human moderators manually reviewing content for possible rule infractions. While users sometimes complain that human moderators are prone to abuses in power or bias, a complaint which sometimes is true, overall human moderators have been proven to be very effective. For example, in 2017, Seering et al. found that when moderators on video streaming site Twitch\index{Twitch} punished bad actors, sometimes by removing them from the video streams, the amount of bad behavior on the moderated video streams decreased in the short term. Another study in 2017 by Chandrasekharan et al. analyzed Reddit moderators' ban of two subreddits (communities on Reddit) called /r/CoonTown and /r/fatpeoplehate, which rather obviously violated Reddit's anti-harassment policy. While the ban caused some users to leave the platform, others began behaving significantly better, and overall the behavior on Reddit improved.

Some communities employ volunteer moderators; however, those communities tend to be smaller in size, often are subcommunities in a larger community, and usually aren't directly run by a profit-driven company. For example, individual Twitch streams, subreddits on Reddit, Facebook groups and self-run Minecraft servers all employ volunteer moderators. Moderators usually volunteer because they believe in the community's goal, because they feel deeply attached to the community, or because they personally know the founder or owner of the community. While volunteer moderation can work for subcommunities, in larger communities, volunteer moderation doesn't typically work because people feel less significant within a larger community.

Larger communities typically pay human moderators, a practice called \keyterm{commercial content moderation}. For example, Facebook pays approximately 15,000 moderators [Newton 2019, theverge.com] and YouTube pays approximately 10,000 moderators [Popper 2017, theverge.com]. As in the case of Henry Soto, these human moderators often face a barrage of horrible content, leaving some of them with psychological problems.

However, we must always remember that moderation\index{moderation} is a thankless task. Users only see the sanitized social system; they don't see all the negative content that moderators had to filter out. Moderation is one type of "invisible labor".\index{moderation!invisible labor}

Many moderators become the target of angry users, who blame them for enforcing the social system's rules, even though in many cases the moderators don't have a say in deciding the policies. "I've been abused for not getting comments down fast enough, for deleting or hiding them, and been called a left-wing ideologue, a right-wing apparatchik [and] a 'f**ing moron'," says Dr. Jennifer Beckett, a former social media moderator for Australian website ABC.

\section{Moderator toolset}

Moderators\index{moderation} have different tools at their disposal to enforce policies and punish offending users.

The most common method of enforcement is to \keyterm{ban}\index{ban} offending users from the social system. There are different ways that moderators can ban people. They can temporarily ban, or \keyterm{tempban}\index{ban!tempban}, offending users for some window of time. This provides punishment for violating the rules but allows the offending user to re-enter the social system later. Moderators can also permanently ban, or \keyterm{permaban}\index{ban!permaban}, users.

If a user is permabanned, they might try to rejoin the social system by setting up an alternate account. This alternate account, called a \keyterm{sockpuppet}\index{sockpuppet}, allows the user to dodge their ban. To combat sockpuppets, moderators will sometimes issue an \keyterm{IP ban}\index{ban!IP ban}, which bans all users from an IP address.

This carries the possibility of banning innocent users who belong to the same IP address as the offending user and should not be done lightly. For example, in 2007, Wikipedia\index{Wikipedia} blocked an IP address based in Qatar from editing Wikipedia in response to "a large volume of spam and vandalism". The problem? That IP address covered the entirety of Qatar. Essentially, Wikipedia blocked the entire country of Qatar, over a million people, from editing Wikipedia [Arrington 2007].\index{ban!IP ban}

Therefore, IP bans should be issued sparingly and only if there seems to be a history of sockpuppeting from an IP address.

IP bans\index{IP bans} are effective in curbing offensive behavior, however. It is technically possible to dodge an IP ban by using a VPN or a proxy server or using a public computer in a different IP address, such as at a school or library. However, this requires the user to go to a lot of trouble, and isn't very common.

What if we want to prevent sockpuppets\index{sockpuppet} but don't want to penalize innocent users who belong to the same IP address. One common strategy is to issue a \keyterm{shadow ban}\index{ban!shadow ban}. A moderator can shadow ban a user by taking away their ability to interact with other users without letting them know that they have lost this ability. A user won't know that they have been shadowbanned and will think they are still able to post or send messages to other users. However, other users won't receive their messages. The offending user won't be able to get a rise out of other users, who will seem to be ignoring him, causing the offending user to become bored of the social system and voluntarily leave.\index{troll}

What if a user has been unfairly banned? Most large social systems have some sort of appeals process for users to plead their case if they believe they have been unfairly banned.\index{ban!appeal}

\subsection{Content review}

How and when do moderators\index{moderation} review content? A social system might prevent content from being posted until it has been reviewed by a moderator. The benefit of this is that users can't see any content that hasn't been approved by a moderator. The downside of this is that this can introduce a long delay between when a user sends content and when other users can see the content. Thus, whether or not to use \keyterm{pre-approval}\index{pre-approval} is a trade-off between maintaining a safe environment and real-time posting. Many family-friendly online forums require all content to be pre-approved by a moderator because maintaining a safe environment is of paramount importance to them. Whereas on many discussion boards like Reddit\index{Reddit}, real-time posting is key and so pre-approval isn't used. To protect against trolls\index{troll}, sometimes discussion boards will require pre-approval on only the first five or ten posts that a user submits. This is enough to ward off most trolls while still allowing dedicated users to submit posts in real-time.

It is also common for social systems to allow users to \keyterm{flag}\index{flagging} content that they believe violates the rules of the social system. A moderator will then review the flagged content and issue a decision about it. Sometimes, the flagged content will be temporarily unavailable while the moderator reviews it. This method ensures that no-one else will view the flagged content, a benefit if the content is graphic or offensive. However, it is also frequently abused by trolls\index{troll} to harass users.

Unlike the pre-approval\index{pre-approval} method, the flagging\index{flagging} method is responsive rather than proactive. With pre-approval, no users view the content before it is reviewed, whereas, with flagging, many users might view the content before it is reviewed, and it might not ever be reviewed if it is not flagged.

\section{Volunteer moderation}

https://www.newyorker.com/tech/annals-of-technology/the-human-toll-of-protecting-the-internet-from-the-worst-of-humanity\\

\section{Community moderation}

Sometimes communities self-moderate. They create \keyterm{blocklists}\index{blocklists}, which are lists of users who a community believe are toxic. They share these lists among other community members, who block the members on the lists. [Geiger 2016] Since these blocklists have no due process, many users on the blocklists feel that they have been unfairly blocked. [Jhaver et al. 2018]\index{moderation!self moderation}

The potential for self-moderating communities to become tyrannies of the majority cannot be overstated. As Alexis de Tocqueville writes in \textit{Democracy in America}:

\begin{displayquote}
    Tyranny in democratic republics does not proceed in the same way, however. It ignores the body and goes straight for the soul. The master no longer says: You will think as I do or die. He says: You are free not to think as I do. You may keep your life, your property, and everything else. But from this day forth you shall be as a stranger among us. You will retain your civic privileges, but they will be of no use to you. For if you seek the votes of your fellow citizens, they will withhold them, and if you seek only their esteem, they will feign to refuse even that. You will remain among men, but you will forfeit your rights to humanity. When you approach your fellow creatures, they will shun you as one who is impure. And even those who believe in your innocence will abandon you, lest they, too, be shunned in turn. Go in peace, I will not take your life, but the life I leave you with is worse than death.
\end{displayquote}

\section{Psychological toll}

\section{Reading questions}
\begin{enumerate}
    \item What is the difference between commercial content moderation and self moderation?
    
    \item Facebook has come under fire for not removing graphic videos of shootings and suicides quickly enough. To solve this problem, in 2017, Mark Zuckerberg announced a plan to hire 3000 more moderators [Chaykowski 2017]. What downsides might result from this decision?
    
    \item Suppose a social media website decides to train a classifier to detect offensive language. The website uses Mechanical Turk to train the classifier, with the racial breakdown of workers being roughly 85\% white and 15\% African American. What problem might this classifier run into?
    
    \item A family-friendly MMO wants to better protect children by preventing them from typing their phone number in the chat. The MMO wants your advice on its proposals.
    \begin{enumerate}
        \item The MMO is thinking about implementing a blacklist and bans all numbers, both in numerical and word form. How might a user get around this blacklist?
        \item Heeding your advice, the MMO instead decides to implement a whitelist. The MMO whitelists all words in the Oxford English Dictionary except words meaning numbers. How might a user get around this whitelist?
        \item The MMO decides to continually expand its list of words which aren't allowed. Any word which users use to mean a number will be banned. Why is this approach problematic for the MMO?
    \end{enumerate}
\end{enumerate}

\section{Solutions to reading questions}
\begin{enumerate}
    \item Commercial content moderation is when a company hires moderators. Self moderation is when a community self-moderates.
    
    \item While removing graphic videos is certainly a good goal, commercial content moderators frequently face psychological harm in their job.
    
    \item Because of the disproportionate amount of white Mechanical Turk workers, the classifier might learn to flag African American Vernacular English. This will cause the algorithmic moderation to bias against African Americans.
    
    \item \begin{enumerate}
        \item The user could misspell the numbers (e.g. onne, twwo, thhree).
        \item The user could use words that look similar to numbers (e.g. won, too, tree).
        \item Removing all words that look similar to numbers will greatly disrupt normal chatting. Users will complain about the difficulty of use and become angry at the MMO.
    \end{enumerate}
\end{enumerate}

\section{Bibliography}

Arrington, Michael. "Wikipedia Bans Qatar." TechCrunch. January 1, 2007.

Chandrasekharan, Eshwar et al. "You Can't Stay Here: The Efficacy of Reddit's 2015 Ban Examined Through Hate Speech." Proceedings of ACM Human-Computer Interaction. 1, 2, Article 31. November 2017.

Chaykowski, Kathleen. "Facebook Is Hiring 3,000 Moderators In Push To Curb Violent Videos." Forbes. May 3, 2017.

Chen, Adrian. "Inside Facebook's Outsourced Anti-Porn and Gore Brigade, Where 'Camel Toes' are More Offensive Than 'Crushed Heads'." Gawker. February 16, 2012.

Chen, Adrian. "When the Internets' Moderators are Anything But." The New York Times Magazine. July, 2015.

Crawford, Kate and Gillespie, Tarleton L. "What is a Flag for? Social Media Reporting Tools and the Vocabulary of Complaint." New Media \& Society. August, 2014.

Geiger, R. Stuart and Ribes, David. "The work of sustaining order in wikipedia: the banning of a vandal." Proceedings of the 2010 ACM conference on Computer supported cooperative work. pp. 117-126. 2010.

Geiger, R. Stuart. "Bot-Based Collective Blocklists in Twitter: The Counterpublic Moderation of Harassment in a Networked Public Space." Information, Communication, and Society 19(6). April 13, 2016.

Gillespie, Tarleton. "Governance of and by platforms." SAGE Handbook of Social Media. 2017.

Isaac, Mike. "Reddit Moderators Shut Down Parts of Site Over Employees Dismissal." The New York Times. July, 2015.

Jennings-Edquist, Grace. "What it's like being an online moderator (and how to make their lives easier)." ABC Life. September 10, 2019.

Jhaver, Shagun et al. "Online Harassment and Content Moderation: The Case of Blocklists." ACM Trans. Computer-Human Interaction. 25, 2, Article 1. March 2018.

Menking, Amanda and Erickson, Ingrid. "The Heart Work of Wikipedia: Gendered, Emotional Labor in the World's Largest Online Encyclopedia." Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. pp. 207-210. 2015.

Newton, Casey. "The Trauma Floor: The secret lives of Facebook moderators in America." The Verge. February 25, 2019.

Popper, Ben. "YouTube CEO promises more moderation to prevent 'bad actors' from 'exploiting our openness'." The Verge. December 4, 2017.

Roberts, Sarah T. "Commercial Content Moderation: Digital Laborers' Dirty Work." Media Studies Publications. 12. 2016.

Seering, Joseph et al. "Shaping Pro and Anti-Social Behavior on Twitch Through Moderation and Example-Setting." Association for Computing Machinery 2017 Conference. 111-125. February 2017.

\textit{Sotos and Blauerts v. Microsoft}, (Florida Superior Court 2016).

Tocqueville, Alexis de., 1838. \textit{Democracy in America}. New York: G. Dearborn \& Co, Chicago.

\end{document}
